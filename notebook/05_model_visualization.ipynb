{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61a8bcc-e121-4ac2-adf1-e0a3927ec13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398a9b28-7194-4e16-94ce-532930daed6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"340pt\" height=\"468pt\"\n",
       " viewBox=\"0.00 0.00 340.00 468.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 464)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-464 336,-464 336,4 -4,4\"/>\n",
       "<!-- 139919441957728 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>139919441957728</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"193.5,-31 139.5,-31 139.5,0 193.5,0 193.5,-31\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n",
       "</g>\n",
       "<!-- 139919437773552 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>139919437773552</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-86 119,-86 119,-67 214,-67 214,-86\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MeanBackward0</text>\n",
       "</g>\n",
       "<!-- 139919437773552&#45;&gt;139919441957728 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>139919437773552&#45;&gt;139919441957728</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.5,-66.79C166.5,-60.07 166.5,-50.4 166.5,-41.34\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170,-41.19 166.5,-31.19 163,-41.19 170,-41.19\"/>\n",
       "</g>\n",
       "<!-- 139919437773744 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>139919437773744</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"217,-141 116,-141 116,-122 217,-122 217,-141\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139919437773744&#45;&gt;139919437773552 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>139919437773744&#45;&gt;139919437773552</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.5,-121.75C166.5,-114.8 166.5,-104.85 166.5,-96.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170,-96.09 166.5,-86.09 163,-96.09 170,-96.09\"/>\n",
       "</g>\n",
       "<!-- 139919437774512 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>139919437774512</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-196 0,-196 0,-177 101,-177 101,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139919437774512&#45;&gt;139919437773744 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>139919437774512&#45;&gt;139919437773744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.14,-176.98C87.8,-168.46 116.75,-155.23 138.24,-145.41\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"139.88,-148.51 147.52,-141.17 136.97,-142.14 139.88,-148.51\"/>\n",
       "</g>\n",
       "<!-- 139919435235888 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>139919435235888</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"80,-262 21,-262 21,-232 80,-232 80,-262\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\">W1.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n",
       "</g>\n",
       "<!-- 139919435235888&#45;&gt;139919437774512 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>139919435235888&#45;&gt;139919437774512</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M50.5,-231.84C50.5,-224.21 50.5,-214.7 50.5,-206.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"54,-206.27 50.5,-196.27 47,-206.27 54,-206.27\"/>\n",
       "</g>\n",
       "<!-- 139919437769904 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>139919437769904</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-196 119,-196 119,-177 214,-177 214,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"166.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">TanhBackward0</text>\n",
       "</g>\n",
       "<!-- 139919437769904&#45;&gt;139919437773744 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>139919437769904&#45;&gt;139919437773744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M166.5,-176.75C166.5,-169.8 166.5,-159.85 166.5,-151.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"170,-151.09 166.5,-141.09 163,-151.09 170,-151.09\"/>\n",
       "</g>\n",
       "<!-- 139922214187408 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>139922214187408</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"213,-256.5 112,-256.5 112,-237.5 213,-237.5 213,-256.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"162.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n",
       "</g>\n",
       "<!-- 139922214187408&#45;&gt;139919437769904 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>139922214187408&#45;&gt;139919437769904</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M163.09,-237.37C163.65,-229.25 164.5,-216.81 165.21,-206.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"168.72,-206.38 165.91,-196.17 161.73,-205.91 168.72,-206.38\"/>\n",
       "</g>\n",
       "<!-- 139919437770288 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>139919437770288</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"119,-322.5 18,-322.5 18,-303.5 119,-303.5 119,-322.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139919437770288&#45;&gt;139922214187408 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>139919437770288&#45;&gt;139922214187408</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M81.18,-303.37C96.59,-292.87 122.85,-275 141.35,-262.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"143.43,-265.22 149.72,-256.7 139.49,-259.43 143.43,-265.22\"/>\n",
       "</g>\n",
       "<!-- 139919435236928 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>139919435236928</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"98,-394 39,-394 39,-364 98,-364 98,-394\"/>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-382\" font-family=\"monospace\" font-size=\"10.00\">W0.bias</text>\n",
       "<text text-anchor=\"middle\" x=\"68.5\" y=\"-371\" font-family=\"monospace\" font-size=\"10.00\"> (16)</text>\n",
       "</g>\n",
       "<!-- 139919435236928&#45;&gt;139919437770288 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>139919435236928&#45;&gt;139919437770288</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M68.5,-363.8C68.5,-354.7 68.5,-342.79 68.5,-332.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"72,-332.84 68.5,-322.84 65,-332.84 72,-332.84\"/>\n",
       "</g>\n",
       "<!-- 139919437770432 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>139919437770432</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-322.5 137,-322.5 137,-303.5 214,-303.5 214,-322.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"175.5\" y=\"-310.5\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139919437770432&#45;&gt;139922214187408 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>139919437770432&#45;&gt;139922214187408</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M173.75,-303.37C171.86,-294.07 168.79,-278.98 166.34,-266.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"169.73,-266.01 164.31,-256.91 162.87,-267.4 169.73,-266.01\"/>\n",
       "</g>\n",
       "<!-- 139919437773456 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>139919437773456</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"226,-388.5 125,-388.5 125,-369.5 226,-369.5 226,-388.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"175.5\" y=\"-376.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139919437773456&#45;&gt;139919437770432 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>139919437773456&#45;&gt;139919437770432</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M175.5,-369.37C175.5,-360.16 175.5,-345.29 175.5,-333.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"179,-332.91 175.5,-322.91 172,-332.91 179,-332.91\"/>\n",
       "</g>\n",
       "<!-- 139919435236528 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>139919435236528</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"211,-460 140,-460 140,-430 211,-430 211,-460\"/>\n",
       "<text text-anchor=\"middle\" x=\"175.5\" y=\"-448\" font-family=\"monospace\" font-size=\"10.00\">W0.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"175.5\" y=\"-437\" font-family=\"monospace\" font-size=\"10.00\"> (16, 8)</text>\n",
       "</g>\n",
       "<!-- 139919435236528&#45;&gt;139919437773456 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>139919435236528&#45;&gt;139919437773456</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M175.5,-429.8C175.5,-420.7 175.5,-408.79 175.5,-398.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"179,-398.84 175.5,-388.84 172,-398.84 179,-398.84\"/>\n",
       "</g>\n",
       "<!-- 139919437774416 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>139919437774416</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"313,-196 236,-196 236,-177 313,-177 313,-196\"/>\n",
       "<text text-anchor=\"middle\" x=\"274.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n",
       "</g>\n",
       "<!-- 139919437774416&#45;&gt;139919437773744 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>139919437774416&#45;&gt;139919437773744</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M257.15,-176.98C239.93,-168.54 213.3,-155.47 193.35,-145.68\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"194.69,-142.43 184.17,-141.17 191.6,-148.72 194.69,-142.43\"/>\n",
       "</g>\n",
       "<!-- 139919437773504 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>139919437773504</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"332,-256.5 231,-256.5 231,-237.5 332,-237.5 332,-256.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"281.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n",
       "</g>\n",
       "<!-- 139919437773504&#45;&gt;139919437774416 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>139919437773504&#45;&gt;139919437774416</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M280.47,-237.37C279.5,-229.25 278.01,-216.81 276.76,-206.39\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"280.2,-205.68 275.54,-196.17 273.25,-206.51 280.2,-205.68\"/>\n",
       "</g>\n",
       "<!-- 139923439956000 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>139923439956000</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"317,-328 246,-328 246,-298 317,-298 317,-328\"/>\n",
       "<text text-anchor=\"middle\" x=\"281.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\">W1.weight</text>\n",
       "<text text-anchor=\"middle\" x=\"281.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\"> (1, 16)</text>\n",
       "</g>\n",
       "<!-- 139923439956000&#45;&gt;139919437773504 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>139923439956000&#45;&gt;139919437773504</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M281.5,-297.8C281.5,-288.7 281.5,-276.79 281.5,-266.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"285,-266.84 281.5,-256.84 278,-266.84 285,-266.84\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f418862aab0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('W0', nn.Linear(8, 16))\n",
    "model.add_module('tanh', nn.Tanh())\n",
    "model.add_module('W1', nn.Linear(16, 1))\n",
    "\n",
    "x = torch.randn(1, 8)\n",
    "y = model(x)\n",
    "\n",
    "make_dot(y.mean(), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5a6066b-30aa-4e3c-b249-9f05248d9654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    \"\"\"LSTM model for forex prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0, bidirectional=False):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Size of input features.\n",
    "            hidden_size (int): Size of hidden layers.\n",
    "            num_layers (int): Number of LSTM layers.\n",
    "            output_size (int): Size of output.\n",
    "            dropout (float, optional): Dropout probability. Defaults to 0.0.\n",
    "            bidirectional (bool, optional): Whether to use bidirectional LSTM. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_type = \"lstm\"\n",
    "        self.meta_data = {\n",
    "            \"input_size\": input_size,\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"output_size\": output_size,\n",
    "            \"dropout\": dropout,\n",
    "            \"bidirectional\": bidirectional\n",
    "        }\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * self.num_directions, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTM model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        # LSTM forward pass\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # lstm_out shape: (batch_size, seq_len, hidden_size * num_directions)\n",
    "        \n",
    "        # Use the last time step output\n",
    "        if self.bidirectional:\n",
    "            lstm_out = lstm_out[:, -1, :]\n",
    "        else:\n",
    "            lstm_out = lstm_out[:, -1, :]\n",
    "        # lstm_out shape: (batch_size, hidden_size * num_directions)\n",
    "        \n",
    "        # Apply dropout\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(lstm_out)\n",
    "        # out shape: (batch_size, output_size)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cb0c471-00d6-42e0-bee3-589f0aacaf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Transformer model for forex prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, num_heads=8, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initialize the Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Size of input features.\n",
    "            hidden_size (int): Size of hidden layers.\n",
    "            num_layers (int): Number of transformer layers.\n",
    "            output_size (int): Size of output.\n",
    "            num_heads (int, optional): Number of attention heads. Defaults to 8.\n",
    "            dropout (float, optional): Dropout probability. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_type = \"transformer\"\n",
    "        self.meta_data = {\n",
    "            \"input_size\": input_size,\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"output_size\": output_size,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"dropout\": dropout\n",
    "        }\n",
    "        \n",
    "        # Embedding layer to convert input to hidden dimension\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(hidden_size, dropout)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_size,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_size * 4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer model.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_size).\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        \n",
    "        # Apply embedding to convert input to hidden dimension\n",
    "        x = self.embedding(x)\n",
    "        # x shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Apply positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        # x shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        transformer_out = self.transformer_encoder(x)\n",
    "        # transformer_out shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Use the last time step output\n",
    "        transformer_out = transformer_out[:, -1, :]\n",
    "        # transformer_out shape: (batch_size, hidden_size)\n",
    "        \n",
    "        # Apply dropout\n",
    "        transformer_out = self.dropout(transformer_out)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(transformer_out)\n",
    "        # out shape: (batch_size, output_size)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for Transformer model.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        \"\"\"\n",
    "        Initialize the positional encoding.\n",
    "        \n",
    "        Args:\n",
    "            d_model (int): Hidden dimension.\n",
    "            dropout (float, optional): Dropout probability. Defaults to 0.1.\n",
    "            max_len (int, optional): Maximum sequence length. Defaults to 5000.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Create positional encoding\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register buffer to store positional encoding\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the positional encoding.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with positional encoding.\n",
    "        \"\"\"\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2af6295-5d60-42a3-9b56-e42dc014bcb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = TransformerModel(3, 64, 1, 5, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ee2ab53-5274-4363-89f7-40b55a58982f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (embedding): Linear(in_features=3, out_features=64, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9ba4ec8-9570-4cd9-8f68-b773ef689c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    \"\"\"GRU model for forex prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.0, bidirectional=False):\n",
    "        \"\"\"\n",
    "        Initialize the GRU model.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Size of input features.\n",
    "            hidden_size (int): Size of hidden layers.\n",
    "            num_layers (int): Number of GRU layers.\n",
    "            output_size (int): Size of output.\n",
    "            dropout (float, optional): Dropout probability. Defaults to 0.0.\n",
    "            bidirectional (bool, optional): Whether to use bidirectional GRU. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model_type = \"gru\"\n",
    "        self.meta_data = {\n",
    "            \"input_size\": input_size,\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"num_layers\": num_layers,\n",
    "            \"output_size\": output_size,\n",
    "            \"dropout\": dropout,\n",
    "            \"bidirectional\": bidirectional\n",
    "        }\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size * self.num_directions, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the GRU model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, output_size).\n",
    "        \"\"\"\n",
    "        # GRU forward pass\n",
    "        # x shape: (batch_size, seq_len, input_size)\n",
    "        gru_out, _ = self.gru(x)\n",
    "        # gru_out shape: (batch_size, seq_len, hidden_size * num_directions)\n",
    "        \n",
    "        # Use the last time step output\n",
    "        gru_out = gru_out[:, -1, :]\n",
    "        # gru_out shape: (batch_size, hidden_size * num_directions)\n",
    "        \n",
    "        # Apply dropout\n",
    "        gru_out = self.dropout(gru_out)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(gru_out)\n",
    "        # out shape: (batch_size, output_size)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d37830eb-90bb-4f87-97a4-35aa59fbdb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRUModel(3, 64, 1, 5, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3f8f950-46ac-4e22-9fce-020b34ee9cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUModel(\n",
      "  (gru): GRU(3, 64, batch_first=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
